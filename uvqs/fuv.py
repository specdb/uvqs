""" Module to ingest SDSS III (aka BOSS) data products
"""
from __future__ import print_function, absolute_import, division, unicode_literals


import numpy as np
import os, json
import pdb

from astropy.table import Table, Column, vstack
from astropy.time import Time
from astropy.io import fits

from linetools import utils as ltu
from linetools.spectra import io as lsio

from specdb.build.utils import chk_for_duplicates
from specdb.build.utils import chk_meta
from specdb.build.utils import init_data


def grab_meta():
    """ Grab meta Table

    Returns
    -------
    boss_meta : Table

    """

    #http://www.sdss.org/dr12/algorithms/boss-dr12-quasar-catalog/
    uvqs_fuv = Table.read(os.getenv('DROPBOX_DIR')+'/z1QSO/database/uvq_dr1_v1.7.fits')
    nuvqs = len(uvqs_fuv)
    # DATE-OBS -- Grab from header
    #t = Time(list(uvqs_fuv['MJD'].data), format='mjd', out_subfmt='date')  # Fixes to YYYY-MM-DD
    #boss_meta.add_column(Column(t.iso, name='DATE-OBS'))
    # Add columns -- From specfiles
    #boss_meta.add_column(Column(['BOSS']*nboss, name='INSTR'))
    #boss_meta.add_column(Column(['BOTH']*nboss, name='GRATING'))
    #boss_meta.add_column(Column([2100.]*nboss, name='R'))  # RESOLUTION
    #boss_meta.add_column(Column(['SDSS 2.5-M']*nboss, name='TELESCOPE'))
    # Redshift logic
    uvqs_fuv['zem_GROUP'] = uvqs_fuv['Z']
    uvqs_fuv['sig_zem'] = uvqs_fuv['Z_SIG']
    uvqs_fuv['flag_zem'] = [str('UVQS')]*nuvqs
    # Rename RA/DEC
    uvqs_fuv.rename_column('RA', 'RA_GROUP')
    uvqs_fuv.rename_column('DEC', 'DEC_GROUP')
    # STYPE
    uvqs_fuv['STYPE'] = [str('QSO')]*nuvqs
    # Check
    assert chk_meta(uvqs_fuv, chk_cat_only=True)
    # Return
    return uvqs_fuv


def hdf5_adddata(hdf, sname, meta, debug=False, chk_meta_only=False, boss_hdf=None, **kwargs):
    """ Add BOSS data to the DB

    Parameters
    ----------
    hdf : hdf5 pointer
    IDs : ndarray
      int array of IGM_ID values in mainDB
    sname : str
      Survey name
    chk_meta_only : bool, optional
      Only check meta file;  will not write
    boss_hdf : str, optional


    Returns
    -------

    """
    # Add Survey
    print("Adding {:s} survey to DB".format(sname))
    if boss_hdf is not None:
        print("Using previously generated {:s} dataset...".format(sname))
        boss_hdf.copy(sname, hdf)
        return
    boss_grp = hdf.create_group(sname)

    # Build spectra (and parse for meta)
    nspec = len(meta)
    max_npix = 4650  # Just needs to be large enough
    data = init_data(max_npix, include_co=True)
    # Init
    spec_set = hdf[sname].create_dataset('spec', data=data, chunks=True,
                                         maxshape=(None,), compression='gzip')
    spec_set.resize((nspec,))
    wvminlist = []
    wvmaxlist = []
    speclist = []
    npixlist = []
    # Loop
    maxpix = 0
    for jj,row in enumerate(meta):
        # Generate full file
        full_file = get_specfil(row)
        if full_file == 'None':
            continue
        # Read
        spec = lsio.readspec(full_file)
        # npix
        npix = spec.npix
        # Kludge for higest redshift systems
        if npix < 10:
            full_file = get_specfil(row, hiz=True)
            try:
                spec = lsio.readspec(full_file)
            except:
                print("Missing: {:s}".format(full_file))
            npix = spec.npix
        elif npix > max_npix:
            raise ValueError("Not enough pixels in the data... ({:d})".format(npix))
        else:
            maxpix = max(npix,maxpix)
        # Parse name
        fname = full_file.split('/')[-1]
        # Fill
        for key in ['wave','flux','sig']:
            data[key] = 0.  # Important to init (for compression too)
        data['flux'][0][:npix] = spec.flux.value
        data['sig'][0][:npix] = spec.sig.value
        data['wave'][0][:npix] = spec.wavelength.value
        # GZ Continuum -- packed in with spectrum, generated by my IDL script
        try:
            co = spec.co.value
        except AttributeError:
            co = np.zeros_like(spec.flux.value)
        # KG Continuum
        KG_file = get_specfil(row, KG=True)
        if os.path.isfile(KG_file) and (npix>1):  # Latter is for junk in GZ file.  Needs fixing
            hduKG = fits.open(KG_file)
            KGtbl = hduKG[1].data
            wvKG = 10.**KGtbl['LOGLAM']
            try:
                assert (wvKG[0]-spec.wavelength[0].value) < 1e-5
            except:
                pdb.set_trace()
            gdpix = np.where(wvKG < (1+row['zem_GROUP'])*1200.)[0]
            co[gdpix] = KGtbl['CONT'][gdpix]
        data['co'][0][:npix] = co
        # Meta
        speclist.append(str(fname))
        wvminlist.append(np.min(data['wave'][0][:npix]))
        wvmaxlist.append(np.max(data['wave'][0][:npix]))
        npixlist.append(npix)
        if chk_meta_only:
            continue
        # Only way to set the dataset correctly
        spec_set[jj] = data

    #
    print("Max pix = {:d}".format(maxpix))
    # Add columns
    meta.add_column(Column(speclist, name='SPEC_FILE'))
    meta.add_column(Column(npixlist, name='NPIX'))
    meta.add_column(Column(wvminlist, name='WV_MIN'))
    meta.add_column(Column(wvmaxlist, name='WV_MAX'))
    meta.add_column(Column(np.arange(nspec,dtype=int),name='GROUP_ID'))
    meta.add_column(Column([2000.]*len(meta), name='EPOCH'))

    # Add HDLLS meta to hdf5
    if chk_meta(meta):
        if chk_meta_only:
            pdb.set_trace()
        hdf[sname]['meta'] = meta
    else:
        pdb.set_trace()
        raise ValueError("meta file failed")
    # References
    refs = [dict(url='http://adsabs.harvard.edu/abs/2015ApJS..219...12A',
                 bib='boss_qso_dr12'),
            ]
    jrefs = ltu.jsonify(refs)
    hdf[sname]['meta'].attrs['Refs'] = json.dumps(jrefs)
    #
    return


def add_ssa(hdf, dset):
    """  Add SSA info to meta dataset
    Parameters
    ----------
    hdf
    dset : str
    """
    from specdb.ssa import default_fields
    Title = '{:s}: BOSS DR12 Quasars'.format(dset)
    ssa_dict = default_fields(Title, flux='flambda', fxcalib='ABSOLUTE')
    hdf[dset]['meta'].attrs['SSA'] = json.dumps(ltu.jsonify(ssa_dict))
